{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a0478c",
   "metadata": {},
   "source": [
    "## get Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b31592ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOWNLOADING FIRST 100 MB OF META-CURRENT DUMP\n",
      "================================================================================\n",
      "URL: https://dumps.wikimedia.org/frwikisource/latest/frwikisource-latest-pages-meta-current.xml.bz2\n",
      "This dump contains expanded/rendered text (transclusions resolved)\n",
      "\n",
      "âœ“ Downloaded 104,857,601 bytes\n",
      "\n",
      "================================================================================\n",
      "DECOMPRESSING BZ2 FILE\n",
      "================================================================================\n",
      "  Decompressed: 100 MB\n",
      "  Decompressed: 200 MB\n",
      "  Decompressed: 300 MB\n",
      "  Decompressed: 400 MB\n",
      "âš  Decompression error: Compressed file ended before the end-of-stream marker was reached\n",
      "  Partial file created: 478,150,656 bytes\n",
      "  Will try to parse what we have...\n",
      "\n",
      "================================================================================\n",
      "CHECKING FOR MISSISSIPPI CONTENT\n",
      "================================================================================\n",
      "âœ“ Found 'Voyage sur le Mississipi' in the dump!\n",
      "\n",
      "Snippet from the article:\n",
      "--------------------------------------------------------------------------------\n",
      "<title>Voyage sur le Mississipi</title>\n",
      "    <ns>0</ns>\n",
      "    <id>21992</id>\n",
      "    <revision>\n",
      "      <id>10644916</id>\n",
      "      <parentid>9805094</parentid>\n",
      "      <timestamp>2020-08-13T20:37:14Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Berniepyt</username>\n",
      "        <id>17489</id>\n",
      "      </contributor>\n",
      "      <minor />\n",
      "      <origin>10644916</origin>\n",
      "      <model>wikitext</model>\n",
      "      <format>text/x-wiki</format>\n",
      "      <text bytes=\"354\" sha1=\"s4r284yf0x4umpi6hzx82nt6cektgdr\" xml:space=\"preserve\">{{TextQuality|Textes validÃ©s}}\n",
      "\n",
      "&lt;pages index=&quot;Revue des Deux Mondes - 1833 - tome 1.djvu&quot; from=531 to=550  header=1 auteur=&quot;[[Auteur:EugÃ¨ne Ney|EugÃ¨ne Ney]]&quot; prev=&quot;&quot; next=&quot;&quot; /&gt;\n",
      "\n",
      "[[CatÃ©gorie:Articles de la Revue des Deux Mondes]]\n",
      "[[CatÃ©gorie:Explorations et voyages aux Ã‰tats-Unis]]\n",
      "[[CatÃ©gorie:Articles par Auteur]]\n",
      "[[CatÃ©gorie:Articles de 1833]]</text>\n",
      "      <sha1>s4r284yf0x4umpi6hzx82nt6cektgdr</sha1>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Auteur:EugÃ¨ne Ney</title>\n",
      "    <ns>102</ns>\n",
      "    <id>21993</id>\n",
      "    <revision>\n",
      "      <id>13635746</id>\n",
      "      <parentid>11883701</parentid>\n",
      "      <timestamp>2023-10-28T16:35:34Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Chrisric</username>\n",
      "        <id>25485</id>\n",
      "      </contributor>\n",
      "      <origin>13635746</origin>\n",
      "      <model>wikitext</model>\n",
      "      <format>text/x-wiki</format>\n",
      "      <text bytes=\"761\" sha1=\"k12jzb9yknbenczxyxautkx7jrc5r0h\" xml:space=\"preserve\">{{Auteur\n",
      "|contenu =\n",
      "\n",
      "== Å’uvres ==\n",
      "{{RDDM}}\n",
      "* [[Voyage en AmÃ©rique]], [[Voyage en AmÃ©rique/01|Terre-Neuve]], {{RDDM2|1|1831}} et [[Voyage en AmÃ©rique/02|Nouvelle-Ã‰cosse]], {{RDDM2|2|1831}} {{4/4}}\n",
      "* [[Lettre sur les Ã‰tats-Unis]], {{RDDM2|2|1831}} {{4/4}}\n",
      "* [[Visite rÃ©cente Ã  lâ€™Ã®le de Cuba]], {{RDDM2|4|1831}} {{4/4}}\n",
      "* [[Excursions dâ€™un officier anglais dans le VÃ©nÃ©zuela|Excursions d'un officier anglais dans le Venezuela, pendant la guerre de lâ€™indÃ©pendance]], {{RDDM2|5|1832}} {{4/4}}\n",
      "* [[Aventures dâ€™un Voyageur amÃ©ricain au milieu des tribus sauvages de la Colombia|Aventures dâ€™un voyageur amÃ©ricain au milieu des tribus sauvages de la Columbia]], {{RDDM2|6|1832}}{{4/4}}\n",
      "* [[Voyage sur le Mississipi]], {{RDDM2|1|1833}} {{4/4}}\n",
      "\n",
      "}}</text>\n",
      "      <sha1>k12jzb9yknbenczxyxautkx7jrc5r0h</sha1>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Auteur:Charles de Montalembert</title>\n",
      "    <ns>102</ns>\n",
      "    <id>21994</id>\n",
      "    <revision>\n",
      "      <id>15165782</id>\n",
      "      <parentid>13660544</parentid>\n",
      "      <timestamp>2025-06-08T19:38:48Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Khardan</username>\n",
      "        <id>13512</id>\n",
      "      </contributor>\n",
      "      <origin>15165782</origin>\n",
      "      <model>wikitext</model>\n",
      "      <format>text/x-wiki</format>\n",
      "      <text bytes=\"1017\" sha1=\"scgr7yeyeau5k03ce2j7gxenbx7lfhw\" xml:space=\"preserve\">{{Auteur\n",
      "| cle = Montalembert, Charles-Forbes-RenÃ© comte de\n",
      "|contenu =\n",
      "\n",
      "== Å’uvres ==\n",
      "{{L2s|Ã€ la mÃ¨re polonaise (traduction Montalembert)||Ã€ la mÃ¨re polonaise|Montalembert - Å’uvres polÃ©miques et diverses de M. l\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ“ Decompressed file ready: frwikisource_meta_partial.xml\n",
      "  Ready to parse with your parser!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Download first 100 MB of the META-CURRENT dump (has expanded text)\n",
    "# ============================================================================\n",
    "url = \"https://dumps.wikimedia.org/frwikisource/latest/frwikisource-latest-pages-meta-current.xml.bz2\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DOWNLOADING FIRST 100 MB OF META-CURRENT DUMP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"URL: {url}\")\n",
    "print(\"This dump contains expanded/rendered text (transclusions resolved)\")\n",
    "print()\n",
    "\n",
    "headers = {'Range': 'bytes=0-104857600'}  # First 100 MB\n",
    "response = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "compressed_file = \"frwikisource_meta_sample.xml.bz2\"\n",
    "decompressed_file = \"frwikisource_meta_partial.xml\"\n",
    "\n",
    "if response.status_code in [200, 206]:\n",
    "    with open(compressed_file, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"âœ“ Downloaded {os.path.getsize(compressed_file):,} bytes\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STEP 2: FULL Decompression of the bz2 file\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DECOMPRESSING BZ2 FILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        compressed_size = os.path.getsize(compressed_file)\n",
    "        bytes_decompressed = 0\n",
    "        \n",
    "        with bz2.open(compressed_file, 'rb') as f_in:\n",
    "            with open(decompressed_file, 'wb') as f_out:\n",
    "                # Read in chunks to handle large files\n",
    "                chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "                while True:\n",
    "                    chunk = f_in.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    f_out.write(chunk)\n",
    "                    bytes_decompressed += len(chunk)\n",
    "                    \n",
    "                    # Show progress every 100 MB\n",
    "                    if bytes_decompressed % (100 * 1024 * 1024) == 0:\n",
    "                        print(f\"  Decompressed: {bytes_decompressed / (1024**2):.0f} MB\")\n",
    "        \n",
    "        decompressed_size = os.path.getsize(decompressed_file)\n",
    "        print(f\"âœ“ Full decompression complete!\")\n",
    "        print(f\"  Compressed:   {compressed_size:,} bytes ({compressed_size / (1024**2):.1f} MB)\")\n",
    "        print(f\"  Decompressed: {decompressed_size:,} bytes ({decompressed_size / (1024**2):.1f} MB)\")\n",
    "        print(f\"  Ratio: {decompressed_size / compressed_size:.1f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš  Decompression error: {e}\")\n",
    "        # Check if we got partial data\n",
    "        if os.path.exists(decompressed_file):\n",
    "            partial_size = os.path.getsize(decompressed_file)\n",
    "            print(f\"  Partial file created: {partial_size:,} bytes\")\n",
    "            print(\"  Will try to parse what we have...\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STEP 3: Quick check - search for Mississippi\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CHECKING FOR MISSISSIPPI CONTENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    with open(decompressed_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "        # Check if Mississippi article is in this partial download\n",
    "        if 'Voyage sur le Mississipi' in content:\n",
    "            print(\"âœ“ Found 'Voyage sur le Mississipi' in the dump!\")\n",
    "            \n",
    "            # Try to extract a snippet\n",
    "            idx = content.find('<title>Voyage sur le Mississipi</title>')\n",
    "            if idx != -1:\n",
    "                # Show 3000 chars starting from title\n",
    "                snippet = content[idx:idx+3000]\n",
    "                print(\"\\nSnippet from the article:\")\n",
    "                print(\"-\" * 80)\n",
    "                print(snippet)\n",
    "                print(\"-\" * 80)\n",
    "        else:\n",
    "            print(\"âœ— 'Voyage sur le Mississipi' not found in this partial download\")\n",
    "            print(\"  You may need to download more data\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Decompressed file ready: {decompressed_file}\")\n",
    "    print(\"  Ready to parse with your parser!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âœ— Download failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185329d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390f3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7620ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading first 10 MB of compressed dump...\n",
      "âœ“ Downloaded 10,485,761 bytes\n",
      "\n",
      "Decompressing bz2 file...\n",
      "âš  Decompression incomplete (expected with partial file): Compressed file ended before the end-of-stream marker was reached\n",
      "Will try to parse what we have...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Download first 10 MB of the compressed dump\n",
    "# ============================================================================\n",
    "url = \"https://dumps.wikimedia.org/frwikisource/latest/frwikisource-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "print(\"Downloading first 10 MB of compressed dump...\")\n",
    "headers = {'Range': 'bytes=0-10485760'}  # First 10 MB\n",
    "response = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "compressed_file = \"frwikisource_sample.xml.bz2\"\n",
    "decompressed_file = \"frwikisource_partial_2.xml\"\n",
    "\n",
    "if response.status_code in [200, 206]:\n",
    "    with open(compressed_file, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"âœ“ Downloaded {os.path.getsize(compressed_file):,} bytes\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STEP 2: Decompress the bz2 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ac372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decompressing bz2 file (FULL decompression)...\n",
      "âš  Decompression error: Compressed file ended before the end-of-stream marker was reached\n",
      "  Partial file created: 50,331,648 bytes\n",
      "  Will try to parse what we have...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FULL DECOMPRESSION\n",
    "# ============================================================================\n",
    "decompressed_file = \"frwikisource_partial_3.xml\"\n",
    "\n",
    "print(\"\\nDecompressing bz2 file (FULL decompression)...\")\n",
    "try:\n",
    "    compressed_size = os.path.getsize(compressed_file)\n",
    "    bytes_decompressed = 0\n",
    "    \n",
    "    with bz2.open(compressed_file, 'rb') as f_in:\n",
    "        with open(decompressed_file, 'wb') as f_out:\n",
    "            # Read in chunks to handle large files\n",
    "            chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "            while True:\n",
    "                chunk = f_in.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f_out.write(chunk)\n",
    "                bytes_decompressed += len(chunk)\n",
    "                \n",
    "                # Show progress every 100 MB\n",
    "                if bytes_decompressed % (100 * 1024 * 1024) == 0:\n",
    "                    print(f\"  Decompressed: {bytes_decompressed / (1024**2):.0f} MB\")\n",
    "    \n",
    "    decompressed_size = os.path.getsize(decompressed_file)\n",
    "    print(f\"âœ“ Full decompression complete!\")\n",
    "    print(f\"  Decompressed to: {decompressed_size:,} bytes ({decompressed_size / (1024**2):.1f} MB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  Decompression error: {e}\")\n",
    "    # Check if we got partial data\n",
    "    if os.path.exists(decompressed_file):\n",
    "        partial_size = os.path.getsize(decompressed_file)\n",
    "        print(f\"  Partial file created: {partial_size:,} bytes\")\n",
    "        print(\"  Will try to parse what we have...\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be7fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 19500 pages...\n",
      "âš  XML parsing stopped (expected with partial file)\n",
      "\n",
      "âœ“ Parsed 19,583 pages total\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_wikisource_dump(dump_file=\"frwikisource_partial.xml\"):\n",
    "    pages = []\n",
    "    \n",
    "    try:\n",
    "        context = ET.iterparse(dump_file, events=('start', 'end'))\n",
    "        context = iter(context)\n",
    "        \n",
    "        event, root = next(context)\n",
    "        \n",
    "        current_page = {}\n",
    "        in_page = False\n",
    "        in_revision = False\n",
    "        \n",
    "        for event, elem in context:\n",
    "            tag = elem.tag.split('}')[-1]\n",
    "            \n",
    "            if event == 'start':\n",
    "                if tag == 'page':\n",
    "                    in_page = True\n",
    "                    current_page = {}\n",
    "                elif tag == 'revision':\n",
    "                    in_revision = True\n",
    "            \n",
    "            elif event == 'end':\n",
    "                if tag == 'title' and in_page:\n",
    "                    current_page['title'] = elem.text\n",
    "                \n",
    "                elif tag == 'ns' and in_page:\n",
    "                    current_page['namespace'] = elem.text\n",
    "                \n",
    "                elif tag == 'id' and in_page and 'page_id' not in current_page:\n",
    "                    current_page['page_id'] = elem.text\n",
    "                \n",
    "                elif tag == 'redirect' and in_page:\n",
    "                    current_page['is_redirect'] = True\n",
    "                    current_page['redirect_to'] = elem.get('title', '')\n",
    "                \n",
    "                elif tag == 'timestamp' and in_revision:\n",
    "                    current_page['timestamp'] = elem.text\n",
    "                \n",
    "                elif tag == 'text' and in_revision:\n",
    "                    current_page['content'] = elem.text if elem.text else ''\n",
    "                \n",
    "                elif tag == 'revision':\n",
    "                    in_revision = False\n",
    "                \n",
    "                elif tag == 'page':\n",
    "                    if 'is_redirect' not in current_page:\n",
    "                        current_page['is_redirect'] = False\n",
    "                        current_page['redirect_to'] = None\n",
    "                    \n",
    "                    # Add Wikisource URL\n",
    "                    if 'title' in current_page:\n",
    "                        title_encoded = current_page['title'].replace(' ', '_')\n",
    "                        current_page['wikisource_url'] = f\"https://fr.wikisource.org/wiki/{title_encoded}\"\n",
    "                    else:\n",
    "                        current_page['wikisource_url'] = None\n",
    "                    \n",
    "                    pages.append(current_page.copy())\n",
    "                    in_page = False\n",
    "                    current_page = {}\n",
    "                    root.clear()\n",
    "                    \n",
    "                    if len(pages) % 100 == 0:\n",
    "                        print(f\"\\rParsed {len(pages)} pages...\", end=\"\")\n",
    "    \n",
    "    except ET.ParseError as e:\n",
    "        print(f\"\\nâš  XML parsing stopped (expected with partial file)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Parsed {len(pages):,} pages total\")\n",
    "    return pd.DataFrame(pages)\n",
    "\n",
    "# Parse the dump with Wikisource URLs\n",
    "df = parse_wikisource_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03194b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Wikisource dump with metadata extraction...\n",
      "Parsed 19500 pages...\n",
      "âš  XML parsing stopped (expected with partial file)\n",
      "\n",
      "âœ“ Parsed 19,583 pages total\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>namespace</th>\n",
       "      <th>page_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "      <th>is_redirect</th>\n",
       "      <th>redirect_to</th>\n",
       "      <th>wikisource_url</th>\n",
       "      <th>extracted_author</th>\n",
       "      <th>extracted_date</th>\n",
       "      <th>extracted_title</th>\n",
       "      <th>extracted_translator</th>\n",
       "      <th>extracted_publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MediaWiki:Monobook.css</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-11-30T23:06:51Z</td>\n",
       "      <td>/* edit this file to customize the monobook sk...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/MediaWiki:Monob...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Espaces wikisources</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MediaWiki:Monobook.js</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-04-13T16:32:56Z</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/MediaWiki:Monob...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MediaWiki:Article</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>2006-01-10T08:38:18Z</td>\n",
       "      <td>Article</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/MediaWiki:Article</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MediaWiki:Categoryarticlecount</td>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "      <td>2005-08-28T21:25:35Z</td>\n",
       "      <td>Il y a $1 Ã©lÃ©ments dans cette catÃ©gorie.</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/MediaWiki:Categ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MediaWiki:Categoryarticlecount1</td>\n",
       "      <td>8</td>\n",
       "      <td>105</td>\n",
       "      <td>2005-08-28T21:24:57Z</td>\n",
       "      <td>Il y a $1 Ã©lÃ©ment dans cette catÃ©gorie.</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/MediaWiki:Categ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>Page:Diderot - Encyclopedie 1ere edition tome ...</td>\n",
       "      <td>104</td>\n",
       "      <td>51901</td>\n",
       "      <td>2024-08-28T17:58:16Z</td>\n",
       "      <td>&lt;noinclude&gt;&lt;pagequality level=\"3\" user=\"AcÃ©lan...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/Page:Diderot_-_...</td>\n",
       "      <td>un conseiller</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19579</th>\n",
       "      <td>Page:Diderot - Encyclopedie 1ere edition tome ...</td>\n",
       "      <td>104</td>\n",
       "      <td>51902</td>\n",
       "      <td>2018-07-23T00:14:52Z</td>\n",
       "      <td>&lt;noinclude&gt;&lt;pagequality level=\"3\" user=\"AcÃ©lan...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/Page:Diderot_-_...</td>\n",
       "      <td>des lettres de dispense</td>\n",
       "      <td>1530</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19580</th>\n",
       "      <td>Page:Diderot - Encyclopedie 1ere edition tome ...</td>\n",
       "      <td>104</td>\n",
       "      <td>51903</td>\n",
       "      <td>2018-07-23T00:14:52Z</td>\n",
       "      <td>&lt;noinclude&gt;&lt;pagequality level=\"3\" user=\"AcÃ©lan...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/Page:Diderot_-_...</td>\n",
       "      <td>rapport aux\\nrangs distincts</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19581</th>\n",
       "      <td>Page:Diderot - Encyclopedie 1ere edition tome ...</td>\n",
       "      <td>104</td>\n",
       "      <td>51904</td>\n",
       "      <td>2024-09-09T04:50:48Z</td>\n",
       "      <td>&lt;noinclude&gt;&lt;pagequality level=\"3\" user=\"AcÃ©lan...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/Page:Diderot_-_...</td>\n",
       "      <td>ledit billet sur une</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>Page:Diderot - Encyclopedie 1ere edition tome ...</td>\n",
       "      <td>104</td>\n",
       "      <td>51905</td>\n",
       "      <td>2018-07-23T00:14:53Z</td>\n",
       "      <td>&lt;noinclude&gt;&lt;pagequality level=\"3\" user=\"AcÃ©lan...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://fr.wikisource.org/wiki/Page:Diderot_-_...</td>\n",
       "      <td>le remettre</td>\n",
       "      <td>1667</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19583 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title namespace page_id  \\\n",
       "0                                 MediaWiki:Monobook.css         8       3   \n",
       "1                                  MediaWiki:Monobook.js         8       4   \n",
       "2                                      MediaWiki:Article         8      54   \n",
       "3                         MediaWiki:Categoryarticlecount         8     104   \n",
       "4                        MediaWiki:Categoryarticlecount1         8     105   \n",
       "...                                                  ...       ...     ...   \n",
       "19578  Page:Diderot - Encyclopedie 1ere edition tome ...       104   51901   \n",
       "19579  Page:Diderot - Encyclopedie 1ere edition tome ...       104   51902   \n",
       "19580  Page:Diderot - Encyclopedie 1ere edition tome ...       104   51903   \n",
       "19581  Page:Diderot - Encyclopedie 1ere edition tome ...       104   51904   \n",
       "19582  Page:Diderot - Encyclopedie 1ere edition tome ...       104   51905   \n",
       "\n",
       "                  timestamp  \\\n",
       "0      2025-11-30T23:06:51Z   \n",
       "1      2019-04-13T16:32:56Z   \n",
       "2      2006-01-10T08:38:18Z   \n",
       "3      2005-08-28T21:25:35Z   \n",
       "4      2005-08-28T21:24:57Z   \n",
       "...                     ...   \n",
       "19578  2024-08-28T17:58:16Z   \n",
       "19579  2018-07-23T00:14:52Z   \n",
       "19580  2018-07-23T00:14:52Z   \n",
       "19581  2024-09-09T04:50:48Z   \n",
       "19582  2018-07-23T00:14:53Z   \n",
       "\n",
       "                                                 content  is_redirect  \\\n",
       "0      /* edit this file to customize the monobook sk...        False   \n",
       "1                                                               False   \n",
       "2                                                Article        False   \n",
       "3               Il y a $1 Ã©lÃ©ments dans cette catÃ©gorie.        False   \n",
       "4                Il y a $1 Ã©lÃ©ment dans cette catÃ©gorie.        False   \n",
       "...                                                  ...          ...   \n",
       "19578  <noinclude><pagequality level=\"3\" user=\"AcÃ©lan...        False   \n",
       "19579  <noinclude><pagequality level=\"3\" user=\"AcÃ©lan...        False   \n",
       "19580  <noinclude><pagequality level=\"3\" user=\"AcÃ©lan...        False   \n",
       "19581  <noinclude><pagequality level=\"3\" user=\"AcÃ©lan...        False   \n",
       "19582  <noinclude><pagequality level=\"3\" user=\"AcÃ©lan...        False   \n",
       "\n",
       "      redirect_to                                     wikisource_url  \\\n",
       "0            None  https://fr.wikisource.org/wiki/MediaWiki:Monob...   \n",
       "1            None  https://fr.wikisource.org/wiki/MediaWiki:Monob...   \n",
       "2            None   https://fr.wikisource.org/wiki/MediaWiki:Article   \n",
       "3            None  https://fr.wikisource.org/wiki/MediaWiki:Categ...   \n",
       "4            None  https://fr.wikisource.org/wiki/MediaWiki:Categ...   \n",
       "...           ...                                                ...   \n",
       "19578        None  https://fr.wikisource.org/wiki/Page:Diderot_-_...   \n",
       "19579        None  https://fr.wikisource.org/wiki/Page:Diderot_-_...   \n",
       "19580        None  https://fr.wikisource.org/wiki/Page:Diderot_-_...   \n",
       "19581        None  https://fr.wikisource.org/wiki/Page:Diderot_-_...   \n",
       "19582        None  https://fr.wikisource.org/wiki/Page:Diderot_-_...   \n",
       "\n",
       "                   extracted_author extracted_date      extracted_title  \\\n",
       "0                              None           None  Espaces wikisources   \n",
       "1                              None           None                 None   \n",
       "2                              None           None                 None   \n",
       "3                              None           None                 None   \n",
       "4                              None           None                 None   \n",
       "...                             ...            ...                  ...   \n",
       "19578                 un conseiller           None                 None   \n",
       "19579       des lettres de dispense           1530                 None   \n",
       "19580  rapport aux\\nrangs distincts           None                 None   \n",
       "19581          ledit billet sur une           None                 None   \n",
       "19582                   le remettre           1667                 None   \n",
       "\n",
       "      extracted_translator extracted_publisher  \n",
       "0                     None                None  \n",
       "1                     None                None  \n",
       "2                     None                None  \n",
       "3                     None                None  \n",
       "4                     None                None  \n",
       "...                    ...                 ...  \n",
       "19578                 None                None  \n",
       "19579                 None                None  \n",
       "19580                 None                None  \n",
       "19581                 None                None  \n",
       "19582                 None                None  \n",
       "\n",
       "[19583 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_metadata(content):\n",
    "    \"\"\"\n",
    "    Extract author, date, and title from Wikisource content.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'extracted_author': None,\n",
    "        'extracted_date': None, \n",
    "        'extracted_title': None,\n",
    "        'extracted_translator': None,\n",
    "        'extracted_publisher': None\n",
    "    }\n",
    "    \n",
    "    if not content or pd.isna(content):\n",
    "        return metadata\n",
    "    \n",
    "    # Look only in first 3000 characters\n",
    "    text = str(content)[:3000]\n",
    "    \n",
    "    # ========== EXTRACT AUTHOR ==========\n",
    "    author_patterns = [\n",
    "        r'Auteur:\\s*\\[\\[([^\\]]+)\\]\\]',           # Auteur:[[Name]]\n",
    "        r'Auteur:\\s*([^\\n\\|}\\]<]+)',             # Auteur:Name\n",
    "        r'auteur\\s*=\\s*\\[\\[([^\\]]+)\\]\\]',        # auteur=[[Name]]\n",
    "        r'auteur\\s*=\\s*([^\\n\\|}\\]<]+)',          # auteur=Name\n",
    "        r'par\\s+([A-ZÃ€-Ãœ][a-zÃ -Ã¼]+(?:\\s+[A-ZÃ€-Ãœ][a-zÃ -Ã¼]+){1,3})',  # par Author Name\n",
    "    ]\n",
    "    \n",
    "    for pattern in author_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            author = match.group(1).strip()\n",
    "            author = re.sub(r'\\|.*$', '', author)\n",
    "            metadata['extracted_author'] = author\n",
    "            break\n",
    "    \n",
    "    # ========== EXTRACT DATE ==========\n",
    "    date_patterns = [\n",
    "        r'annÃ©e\\s*=\\s*(\\d{4})',\n",
    "        r'date\\s*=\\s*(\\d{4})',\n",
    "        r'Ã©diteur,?\\s+(\\d{4})',\n",
    "        r'\\b(1[0-9]{3}|20[0-2][0-9])\\b',\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            metadata['extracted_date'] = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # ========== EXTRACT TITLE ==========\n",
    "    title_patterns = [\n",
    "        r'titre\\s*=\\s*([^\\n\\|}\\]<]+)',\n",
    "        r'title\\s*=\\s*([^\\n\\|}\\]<]+)',\n",
    "        r'==\\s*([^=\\n]+?)\\s*==',\n",
    "        r'<h[12]>([^<]+)</h[12]>',\n",
    "    ]\n",
    "    \n",
    "    for pattern in title_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            title = match.group(1).strip()\n",
    "            if title.upper() not in ['INTRODUCTION', 'PRÃ‰FACE', 'AVANT-PROPOS', \n",
    "                                     'SOMMAIRE', 'TABLE DES MATIÃˆRES', 'INDEX']:\n",
    "                metadata['extracted_title'] = title\n",
    "                break\n",
    "    \n",
    "    # ========== EXTRACT TRANSLATOR ==========\n",
    "    translator_patterns = [\n",
    "        r'Traduction par\\s+\\[\\[([^\\]]+)\\]\\]',\n",
    "        r'Traduction par\\s+([A-ZÃ€-Ãœ][^\\n\\.,<]+)',\n",
    "        r'traducteur\\s*=\\s*\\[\\[([^\\]]+)\\]\\]',\n",
    "        r'traducteur\\s*=\\s*([^\\n\\|}\\]<]+)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in translator_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            translator = match.group(1).strip()\n",
    "            translator = re.sub(r'\\|.*$', '', translator)\n",
    "            metadata['extracted_translator'] = translator\n",
    "            break\n",
    "    \n",
    "    # ========== EXTRACT PUBLISHER ==========\n",
    "    publisher_patterns = [\n",
    "        r'Ã©diteur\\s*=\\s*([^\\n\\|}\\]<]+)',\n",
    "        r'\\[\\[([^\\]]+)\\]\\],?\\s*Ã©diteur',\n",
    "        r'([A-ZÃ€-Ãœ][a-zÃ -Ã¼]+),?\\s*Ã©diteur',\n",
    "    ]\n",
    "    \n",
    "    for pattern in publisher_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            publisher = match.group(1).strip()\n",
    "            publisher = re.sub(r'\\|.*$', '', publisher)\n",
    "            metadata['extracted_publisher'] = publisher\n",
    "            break\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def parse_wikisource_dump(dump_file=\"frwikisource_partial.xml\"):\n",
    "    \"\"\"\n",
    "    Parse Wikisource dump and extract metadata from content.\n",
    "    \"\"\"\n",
    "    pages = []\n",
    "    \n",
    "    try:\n",
    "        context = ET.iterparse(dump_file, events=('start', 'end'))\n",
    "        context = iter(context)\n",
    "        \n",
    "        event, root = next(context)\n",
    "        \n",
    "        current_page = {}\n",
    "        in_page = False\n",
    "        in_revision = False\n",
    "        \n",
    "        for event, elem in context:\n",
    "            tag = elem.tag.split('}')[-1]\n",
    "            \n",
    "            if event == 'start':\n",
    "                if tag == 'page':\n",
    "                    in_page = True\n",
    "                    current_page = {}\n",
    "                elif tag == 'revision':\n",
    "                    in_revision = True\n",
    "            \n",
    "            elif event == 'end':\n",
    "                if tag == 'title' and in_page:\n",
    "                    current_page['title'] = elem.text\n",
    "                \n",
    "                elif tag == 'ns' and in_page:\n",
    "                    current_page['namespace'] = elem.text\n",
    "                \n",
    "                elif tag == 'id' and in_page and 'page_id' not in current_page:\n",
    "                    current_page['page_id'] = elem.text\n",
    "                \n",
    "                elif tag == 'redirect' and in_page:\n",
    "                    current_page['is_redirect'] = True\n",
    "                    current_page['redirect_to'] = elem.get('title', '')\n",
    "                \n",
    "                elif tag == 'timestamp' and in_revision:\n",
    "                    current_page['timestamp'] = elem.text\n",
    "                \n",
    "                elif tag == 'text' and in_revision:\n",
    "                    current_page['content'] = elem.text if elem.text else ''\n",
    "                \n",
    "                elif tag == 'revision':\n",
    "                    in_revision = False\n",
    "                \n",
    "                elif tag == 'page':\n",
    "                    if 'is_redirect' not in current_page:\n",
    "                        current_page['is_redirect'] = False\n",
    "                        current_page['redirect_to'] = None\n",
    "                    \n",
    "                    # Add Wikisource URL\n",
    "                    if 'title' in current_page:\n",
    "                        title_encoded = current_page['title'].replace(' ', '_')\n",
    "                        current_page['wikisource_url'] = f\"https://fr.wikisource.org/wiki/{title_encoded}\"\n",
    "                    else:\n",
    "                        current_page['wikisource_url'] = None\n",
    "                    \n",
    "                    # Extract metadata from content\n",
    "                    if 'content' in current_page:\n",
    "                        metadata = extract_metadata(current_page['content'])\n",
    "                        current_page.update(metadata)\n",
    "                    \n",
    "                    pages.append(current_page.copy())\n",
    "                    in_page = False\n",
    "                    current_page = {}\n",
    "                    root.clear()\n",
    "                    \n",
    "                    if len(pages) % 100 == 0:\n",
    "                        print(f\"\\rParsed {len(pages)} pages...\", end=\"\")\n",
    "    \n",
    "    except ET.ParseError as e:\n",
    "        print(f\"\\nâš  XML parsing stopped (expected with partial file)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Parsed {len(pages):,} pages total\")\n",
    "    return pd.DataFrame(pages)\n",
    "\n",
    "# Parse the dump with metadata extraction\n",
    "print(\"Parsing Wikisource dump with metadata extraction...\")\n",
    "df = parse_wikisource_dump()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e769885c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{TextQuality|Textes validÃ©s}}\\n\\n<pages index=\"Revue des Deux Mondes - 1833 - tome 1.djvu\" from=531 to=550  header=1 auteur=\"[[Auteur:EugÃ¨ne Ney|EugÃ¨ne Ney]]\" prev=\"\" next=\"\" />\\n\\n[[CatÃ©gorie:Articles de la Revue des Deux Mondes]]\\n[[CatÃ©gorie:Explorations et voyages aux Ã‰tats-Unis]]\\n[[CatÃ©gorie:Articles par Auteur]]\\n[[CatÃ©gorie:Articles de 1833]]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['wikisource_url']=='https://fr.wikisource.org/wiki/Voyage_sur_le_Mississipi']['content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b11682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wikisource_pages_with_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ff181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['is_redirect']==False]\n",
    "df['content_length'] = df['content'].fillna('').str.len()\n",
    "df = df[df['content_length'] >= 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ff4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FRENCH WIKISOURCE DATA\n",
      "============================================================\n",
      "\n",
      "Total pages: 9,702\n",
      "\n",
      "Namespace breakdown:\n",
      "namespace\n",
      "0      5288\n",
      "104    3360\n",
      "102     758\n",
      "14      114\n",
      "10       64\n",
      "4        43\n",
      "6        26\n",
      "106      24\n",
      "12       12\n",
      "8         8\n",
      "112       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Main namespace articles: 5,288\n",
      "\n",
      "ðŸ“‹ First 10 articles:\n",
      "   page_id                                              title\n",
      "19    1309  Charte internationale des Droits de lâ€™Homme (1...\n",
      "20    1310  Constitution du 4 octobre 1958 (Ã  jour de la r...\n",
      "21    1311  DÃ©claration des Droits de lâ€™Homme et du Citoye...\n",
      "27    1318         Les Fleurs du mal (1861)/Lâ€™Homme et la Mer\n",
      "28    1319                          Pour le bien-Ãªtre de tous\n",
      "29    1320    Commentaires sur Unto This Last de M. K. Gandhi\n",
      "30    1322  DÃ©cret de la Convention nationale portant sur ...\n",
      "32    1329  Constitution des Ã‰tats-Unis dâ€™AmÃ©rique (trad. ...\n",
      "38    1342                                  PremiÃ¨re Solitude\n",
      "39    1343                                  Aux amis inconnus\n",
      "\n",
      "============================================================\n",
      "STEP 4: Fetching Wikidata IDs (first 20 articles)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying Wikidata:   5%|â–Œ         | 1/20 [00:30<09:40, 30.57s/it]"
     ]
    }
   ],
   "source": [
    "# Show info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FRENCH WIKISOURCE DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal pages: {len(df):,}\")\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nNamespace breakdown:\")\n",
    "    print(df['namespace'].value_counts())\n",
    "    \n",
    "    # Filter main namespace only (ns=0)\n",
    "    df_main = df[df['namespace'] == '0'].copy()\n",
    "    print(f\"\\nMain namespace articles: {len(df_main):,}\")\n",
    "    \n",
    "    print(\"\\nðŸ“‹ First 10 articles:\")\n",
    "    print(df_main[['page_id', 'title']].head(10))\n",
    "    \n",
    "    # STEP 4: Get Wikidata IDs\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 4: Fetching Wikidata IDs (first 20 articles)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def get_wikidata_id_from_wikisource(wikisource_url):\n",
    "        \"\"\"\n",
    "        Get Wikidata ID for a Wikisource page.\n",
    "        Handles redirects and different apostrophe characters.\n",
    "        \"\"\"\n",
    "        headers = {'User-Agent': 'WikidataBot/1.0 (Python requests)'}\n",
    "        \n",
    "        # Extract page title from URL\n",
    "        parts = wikisource_url.split('/wiki/')\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        page_title = urllib.parse.unquote(parts[1])\n",
    "        \n",
    "        # Determine the wiki site from URL (e.g., 'fr' from 'fr.wikisource.org')\n",
    "        wiki_site = wikisource_url.split('//')[1].split('.')[0] + 'wikisource'\n",
    "        api_base = f\"https://{wikisource_url.split('//')[1].split('/')[0]}/w/api.php\"\n",
    "        \n",
    "        # Step 1: Follow redirects to get the actual page title\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'titles': page_title,\n",
    "            'redirects': '',\n",
    "            'prop': 'pageprops',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(api_base, params=params, headers=headers, timeout=15)\n",
    "            data = r.json()\n",
    "            \n",
    "            # Check for redirects\n",
    "            redirects = data.get('query', {}).get('redirects', [])\n",
    "            if redirects:\n",
    "                page_title = redirects[-1]['to']  # Get final redirect target\n",
    "            \n",
    "            # Check if wikibase_item is in pageprops\n",
    "            pages = data.get('query', {}).get('pages', {})\n",
    "            for page_id, page_data in pages.items():\n",
    "                if page_id != '-1':\n",
    "                    wikibase_item = page_data.get('pageprops', {}).get('wikibase_item')\n",
    "                    if wikibase_item:\n",
    "                        return wikibase_item\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Step 2: Query Wikidata directly using the (possibly redirected) title\n",
    "        try:\n",
    "            wd_api_url = \"https://www.wikidata.org/w/api.php\"\n",
    "            params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'sites': wiki_site,\n",
    "                'titles': page_title,\n",
    "                'format': 'json'\n",
    "            }\n",
    "            \n",
    "            r = requests.get(wd_api_url, params=params, headers=headers, timeout=15)\n",
    "            data = r.json()\n",
    "            \n",
    "            entities = data.get('entities', {})\n",
    "            for entity_id, entity_data in entities.items():\n",
    "                if entity_id != '-1' and not entity_id.startswith('-'):\n",
    "                    return entity_id\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    wikidata_ids = []\n",
    "    \n",
    "    for idx, row in tqdm(df_main.head(20).iterrows(), total=min(20, len(df_main)), desc=\"Querying Wikidata\"):\n",
    "        wikisource_url = row['wikisource_url']\n",
    "        \n",
    "        wikidata_id = get_wikidata_id_from_wikisource(wikisource_url)\n",
    "        \n",
    "        wikidata_ids.append({\n",
    "            'page_id': row['page_id'],\n",
    "            'title': row['title'],\n",
    "            'wikisource_url': wikisource_url,\n",
    "            'wikidata_id': wikidata_id\n",
    "        })\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    results_df = pd.DataFrame(wikidata_ids)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS: Wikidata IDs Found\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTotal pages checked: {len(results_df)}\")\n",
    "    print(f\"Pages with Wikidata ID: {len(results_df[results_df['wikidata_id'].notna()])}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Pages with Wikidata IDs:\")\n",
    "    for idx, row in results_df[results_df['wikidata_id'].notna()].iterrows():\n",
    "        print(f\"\\nâ€¢ {row['title']}\")\n",
    "        print(f\"  Wikisource: {row['wikisource_url']}\")\n",
    "        print(f\"  Wikidata ID: {row['wikidata_id']}\")\n",
    "    \n",
    "    results_df.to_csv('frwikisource_wikidata_ids.csv', index=False)\n",
    "    print(f\"\\nâœ“ Saved to frwikisource_wikidata_ids.csv\")\n",
    "else:\n",
    "    print(\"âš  No pages were parsed. Try downloading more data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28a61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikidata ID: None\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
